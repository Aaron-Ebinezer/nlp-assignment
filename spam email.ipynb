{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35c7bb09-34da-47b4-8e34-18315f6dad33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\aaaro\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\aaaro\\anaconda3\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: click in c:\\users\\aaaro\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\aaaro\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\aaaro\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\aaaro\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\aaaro\\anaconda3\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\aaaro\\anaconda3\\lib\\site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\aaaro\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\aaaro\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk scikit-learn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db0000ef-5692-431f-8cc5-9b83e7c33996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label                                               text\n",
      "0    1.0  ounce feather bowl hummingbird opec moment ala...\n",
      "1    1.0  wulvob get your medircations online qnb ikud v...\n",
      "2    0.0   computer connection from cnn com wednesday es...\n",
      "3    1.0  university degree obtain a prosperous future m...\n",
      "4    0.0  thanks for all your answers guys i know i shou...\n",
      "Training set shape: (37, 588)\n",
      "Test set shape: (10, 588)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aaaro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\aaaro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load the dataset\n",
    "# Make sure your dataset has 'text' and 'label' columns\n",
    "df = pd.read_csv(r\"C:\\study materials\\data preprocessing.csv\")\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(df.head())\n",
    "\n",
    "# Fill NaN values with empty string and ensure all entries are strings\n",
    "df['text'] = df['text'].fillna('').astype(str)\n",
    "\n",
    "# Text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n",
    "    text = re.sub(r'\\^[a-zA-Z]\\s+', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text, flags=re.I)\n",
    "    text = re.sub(r'^b\\s+', '', text)\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenization and lemmatization\n",
    "    tokens = text.split()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply the preprocessing function to the text column\n",
    "df['processed_text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['processed_text'] = df['processed_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['processed_text'], df['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert text data to numerical data using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Save the vectorizer for later use\n",
    "import pickle\n",
    "with open('tfidf_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidf_vectorizer, f)\n",
    "\n",
    "# Display the shapes of the resulting matrices\n",
    "print(\"Training set shape:\", X_train_tfidf.shape)\n",
    "print(\"Test set shape:\", X_test_tfidf.shape)\n",
    "\n",
    "# The processed data is now ready for model training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5959a65e-a409-4287-bb61-d35374989f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aaaro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\aaaro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label                                               text\n",
      "0      1  ounce feather bowl hummingbird opec moment ala...\n",
      "1      1  wulvob get your medircations online qnb ikud v...\n",
      "2      0   computer connection from cnn com wednesday es...\n",
      "3      1  university degree obtain a prosperous future m...\n",
      "4      0  thanks for all your answers guys i know i shou...\n",
      "Number of missing labels: 0\n",
      "Training set shape after feature selection: (66758, 1000)\n",
      "Test set shape after feature selection: (16690, 1000)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pickle\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\aaaro\\Downloads\\archive (4)\\combined_data.csv\")\n",
    "# Display the first few rows of the dataset\n",
    "print(df.head())\n",
    "\n",
    "# Fill NaN values with empty string and ensure all entries are strings\n",
    "df['text'] = df['text'].fillna('').astype(str)\n",
    "\n",
    "# Text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n",
    "    text = re.sub(r'\\^[a-zA-Z]\\s+', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text, flags=re.I)\n",
    "    text = re.sub(r'^b\\s+', '', text)\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenization and lemmatization\n",
    "    tokens = text.split()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply the preprocessing function to the text column\n",
    "df['processed_text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['processed_text'] = df['processed_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "\n",
    "# Check for missing values in the label column\n",
    "missing_labels = df['label'].isna().sum()\n",
    "print(f\"Number of missing labels: {missing_labels}\")\n",
    "\n",
    "# Option 1: Drop rows with missing labels\n",
    "df = df.dropna(subset=['label'])\n",
    "\n",
    "# Option 2: Replace NaN labels with a default value (if applicable)\n",
    "# df['label'] = df['label'].fillna('default_label')  # Replace 'default_label' with an appropriate value\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['processed_text'], df['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature extraction using TF-IDF and feature selection with chi2\n",
    "k = 1000  # Number of features to select\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=5000)),\n",
    "    ('chi2', SelectKBest(chi2, k=k))\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the training data and transform both training and test sets\n",
    "X_train_selected = pipeline.fit_transform(X_train, y_train)\n",
    "X_test_selected = pipeline.transform(X_test)\n",
    "\n",
    "# Save the pipeline for later use\n",
    "with open('tfidf_chi2_pipeline.pkl', 'wb') as f:\n",
    "    pickle.dump(pipeline, f)\n",
    "\n",
    "# Display the shapes of the resulting matrices\n",
    "print(\"Training set shape after feature selection:\", X_train_selected.shape)\n",
    "print(\"Test set shape after feature selection:\", X_test_selected.shape)\n",
    "\n",
    "# The processed data is now ready for model training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0304192e-c191-42b1-b1a3-b90bb80e766a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 97.33%\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97      7938\n",
      "           1       0.97      0.98      0.97      8752\n",
      "\n",
      "    accuracy                           0.97     16690\n",
      "   macro avg       0.97      0.97      0.97     16690\n",
      "weighted avg       0.97      0.97      0.97     16690\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[7629  309]\n",
      " [ 136 8616]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aaaro\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Define the model\n",
    "model = LogisticRegression()\n",
    "-\n",
    "# Train the model on the selected features\n",
    "model.fit(X_train_selected, y_train)\n",
    "\n",
    "# Save the model for later use\n",
    "with open('spam_classifier_model.pkl', 'wb') as model_file:\n",
    "    pickle.dump(model, model_file)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test_selected)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"\\nClassification Report:\\n\", report)\n",
    "\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8206fa9e-d2b6-49ee-8407-14e2030088af",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "X_new: This should be the new input data that you want to classify. In the example above, X_new is a list of strings representing new email texts.\n",
    "\n",
    "Transform the new data: The loaded_pipeline.transform(X_new) step applies the same preprocessing steps to the new data as were applied during training.\n",
    "\n",
    "Predict using the model: The loaded_model.predict(X_new_selected) step uses the trained model to predict the class (spam or not spam) for the new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb543a67-b377-4e00-8b5f-eb4973dc8c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the feature extraction pipeline\n",
    "with open('tfidf_chi2_pipeline.pkl', 'rb') as file:\n",
    "    loaded_pipeline = pickle.load(file)\n",
    "\n",
    "# Load the trained model\n",
    "with open('spam_classifier_model.pkl', 'rb') as model_file:\n",
    "    loaded_model = pickle.load(model_file)\n",
    "X_new = [\"Congratulations! You've won a free ticket to the Bahamas!\", \n",
    "         \"Don't miss our summer sale on all electronics.\",\n",
    "         \"This is a regular email without any spammy content.\",\n",
    "        \"Your request for an Amazon SageMaker Studio Lab expired because you didnâ€™t register your account within 7 days of approval\"\n",
    "]\n",
    "\n",
    "# Use the loaded pipeline and model\n",
    "X_new_selected = loaded_pipeline.transform(X_new)  # Apply to new data\n",
    "y_new_pred = loaded_model.predict(X_new_selected)\n",
    "\n",
    "# Output predictions\n",
    "print(y_new_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3f6fc4-400a-4b51-a0d4-def12c12355c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
